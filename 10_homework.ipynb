{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchtune torchao -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:23:38.114940Z","iopub.execute_input":"2025-04-02T13:23:38.115320Z","iopub.status.idle":"2025-04-02T13:23:41.604620Z","shell.execute_reply.started":"2025-04-02T13:23:38.115293Z","shell.execute_reply":"2025-04-02T13:23:41.603559Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data\n\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom tokenizers.trainers import BpeTrainer\nfrom tokenizers import decoders\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\nimport re\nimport os\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.model_selection import StratifiedShuffleSplit, train_test_split\nfrom collections import Counter\n\nfrom torchtune.modules import RotaryPositionalEmbeddings\nfrom torch.nn import Transformer\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:23:41.605865Z","iopub.execute_input":"2025-04-02T13:23:41.606127Z","iopub.status.idle":"2025-04-02T13:23:41.613715Z","shell.execute_reply.started":"2025-04-02T13:23:41.606090Z","shell.execute_reply":"2025-04-02T13:23:41.612856Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:23:42.742783Z","iopub.execute_input":"2025-04-02T13:23:42.743079Z","iopub.status.idle":"2025-04-02T13:23:42.746824Z","shell.execute_reply.started":"2025-04-02T13:23:42.743057Z","shell.execute_reply":"2025-04-02T13:23:42.746194Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru\n!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:23:43.905016Z","iopub.execute_input":"2025-04-02T13:23:43.905391Z","iopub.status.idle":"2025-04-02T13:23:54.261143Z","shell.execute_reply.started":"2025-04-02T13:23:43.905364Z","shell.execute_reply":"2025-04-02T13:23:54.260306Z"}},"outputs":[{"name":"stdout","text":"--2025-04-02 13:23:43--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\nResolving data.statmt.org (data.statmt.org)... 129.215.32.28\nConnecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 121340806 (116M)\nSaving to: ‘opus.en-ru-train.ru.1’\n\nopus.en-ru-train.ru 100%[===================>] 115.72M  31.6MB/s    in 4.3s    \n\n2025-04-02 13:23:48 (27.1 MB/s) - ‘opus.en-ru-train.ru.1’ saved [121340806/121340806]\n\n--2025-04-02 13:23:48--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\nResolving data.statmt.org (data.statmt.org)... 129.215.32.28\nConnecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 67760131 (65M)\nSaving to: ‘opus.en-ru-train.en.1’\n\nopus.en-ru-train.en 100%[===================>]  64.62M  23.4MB/s    in 2.8s    \n\n2025-04-02 13:23:52 (23.4 MB/s) - ‘opus.en-ru-train.en.1’ saved [67760131/67760131]\n\n--2025-04-02 13:23:52--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru\nResolving data.statmt.org (data.statmt.org)... 129.215.32.28\nConnecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 305669 (299K)\nSaving to: ‘opus.en-ru-test.ru.1’\n\nopus.en-ru-test.ru. 100%[===================>] 298.50K   691KB/s    in 0.4s    \n\n2025-04-02 13:23:53 (691 KB/s) - ‘opus.en-ru-test.ru.1’ saved [305669/305669]\n\n--2025-04-02 13:23:53--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en\nResolving data.statmt.org (data.statmt.org)... 129.215.32.28\nConnecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 173307 (169K)\nSaving to: ‘opus.en-ru-test.en.1’\n\nopus.en-ru-test.en. 100%[===================>] 169.25K   532KB/s    in 0.3s    \n\n2025-04-02 13:23:54 (532 KB/s) - ‘opus.en-ru-test.en.1’ saved [173307/173307]\n\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"text = open('opus.en-ru-train.ru').read().replace('\\xa0', ' ')\nf = open('opus.en-ru-train.ru', 'w')\nf.write(text)\nf.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:23:54.262416Z","iopub.execute_input":"2025-04-02T13:23:54.262663Z","iopub.status.idle":"2025-04-02T13:23:56.028076Z","shell.execute_reply.started":"2025-04-02T13:23:54.262641Z","shell.execute_reply":"2025-04-02T13:23:56.027092Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"en_sents = open('opus.en-ru-train.en').read().splitlines()\nru_sents = open('opus.en-ru-train.ru').read().splitlines()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:23:56.029789Z","iopub.execute_input":"2025-04-02T13:23:56.030134Z","iopub.status.idle":"2025-04-02T13:23:59.135166Z","shell.execute_reply.started":"2025-04-02T13:23:56.030084Z","shell.execute_reply":"2025-04-02T13:23:59.134454Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"tokenizer_en = Tokenizer(BPE())\ntokenizer_en.pre_tokenizer = Whitespace()\n\ntrainer_en = BpeTrainer(special_tokens=[\"[PAD]\", \"[BOS]\", \"[EOS]\"], end_of_word_suffix='</w>')\ntokenizer_en.train(files=[\"opus.en-ru-train.en\"], trainer=trainer_en)\n\ntokenizer_ru = Tokenizer(BPE())\ntokenizer_ru.pre_tokenizer = Whitespace()\n\ntrainer_ru = BpeTrainer(special_tokens=[\"[PAD]\"], end_of_word_suffix='</w>')\ntokenizer_ru.train(files=[\"opus.en-ru-train.ru\"], trainer=trainer_ru)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:23:59.136931Z","iopub.execute_input":"2025-04-02T13:23:59.137253Z","iopub.status.idle":"2025-04-02T13:24:24.747816Z","shell.execute_reply.started":"2025-04-02T13:23:59.137229Z","shell.execute_reply":"2025-04-02T13:24:24.747148Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"tokenizer_en.decoder = decoders.BPEDecoder()\ntokenizer_ru.decoder = decoders.BPEDecoder()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:24:24.756391Z","iopub.execute_input":"2025-04-02T13:24:24.756608Z","iopub.status.idle":"2025-04-02T13:24:24.759941Z","shell.execute_reply.started":"2025-04-02T13:24:24.756590Z","shell.execute_reply":"2025-04-02T13:24:24.759284Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"def encode(text, tokenizer, max_len, encoder=False):\n    if encoder:\n        return tokenizer.encode(text).ids[:max_len]\n    else:\n        return [tokenizer.token_to_id('[BOS]')] + tokenizer.encode(text).ids[:max_len] + [tokenizer.token_to_id('[EOS]')]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:24:24.760717Z","iopub.execute_input":"2025-04-02T13:24:24.760918Z","iopub.status.idle":"2025-04-02T13:24:24.777195Z","shell.execute_reply.started":"2025-04-02T13:24:24.760889Z","shell.execute_reply":"2025-04-02T13:24:24.776447Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"PAD_IDX = tokenizer_en.token_to_id('[PAD]')\nmax_len_en, max_len_ru = 47, 48 \nX_ru = [encode(t, tokenizer_ru, max_len_ru, encoder=True) for t in ru_sents]\nX_en = [encode(t, tokenizer_en, max_len_en) for t in en_sents]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:24:24.779297Z","iopub.execute_input":"2025-04-02T13:24:24.779494Z","iopub.status.idle":"2025-04-02T13:25:38.076365Z","shell.execute_reply.started":"2025-04-02T13:24:24.779477Z","shell.execute_reply":"2025-04-02T13:25:38.075611Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, texts_ru, texts_en):\n        self.texts_ru = [torch.LongTensor(sent) for sent in texts_ru]\n        self.texts_ru = torch.nn.utils.rnn.pad_sequence(self.texts_ru, batch_first=True, padding_value=PAD_IDX)\n\n        self.texts_en = [torch.LongTensor(sent) for sent in texts_en]\n        self.texts_en = torch.nn.utils.rnn.pad_sequence(self.texts_en, batch_first=True, padding_value=PAD_IDX)\n\n        self.length = len(texts_ru)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, index):\n        ids_ru = self.texts_ru[index]\n        ids_en = self.texts_en[index]\n        return ids_ru, ids_en","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:25:38.077429Z","iopub.execute_input":"2025-04-02T13:25:38.077668Z","iopub.status.idle":"2025-04-02T13:25:38.083013Z","shell.execute_reply.started":"2025-04-02T13:25:38.077645Z","shell.execute_reply":"2025-04-02T13:25:38.082201Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"X_ru_train, X_ru_valid, X_en_train, X_en_valid = train_test_split(X_ru, X_en, test_size=0.05)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:25:38.083862Z","iopub.execute_input":"2025-04-02T13:25:38.084194Z","iopub.status.idle":"2025-04-02T13:25:40.478737Z","shell.execute_reply.started":"2025-04-02T13:25:38.084159Z","shell.execute_reply":"2025-04-02T13:25:40.477789Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"class TransformerEncoderDecoder(nn.Module):\n    def __init__(self, vocab_size_enc, vocab_size_dec, embed_dim, num_heads, ff_dim, num_layers, dropout=0.1):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.embedding_enc = nn.Embedding(vocab_size_enc, embed_dim)\n        self.embedding_dec = nn.Embedding(vocab_size_dec, embed_dim)\n        self.positional_encoding = RotaryPositionalEmbeddings(embed_dim // num_heads, max_seq_len=128)\n\n        self.transformer = Transformer(\n            d_model=embed_dim,\n            nhead=num_heads,\n            num_encoder_layers=num_layers,\n            num_decoder_layers=num_layers,\n            dim_feedforward=ff_dim,\n            dropout=dropout,\n            batch_first=True\n        )\n\n        self.output_layer = nn.Linear(embed_dim, vocab_size_dec)\n\n    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None):\n\n        src_embedded = self.embedding_enc(src)\n        B,S,E = src_embedded.shape\n        src_embedded = self.positional_encoding(src_embedded.view(B,S,self.num_heads, E//self.num_heads)).view(B,S,E)\n\n        tgt_embedded = self.embedding_dec(tgt)\n        B,S,E = tgt_embedded.shape\n        tgt_embedded = self.positional_encoding(tgt_embedded.view(B,S,self.num_heads, E//self.num_heads)).view(B,S,E)\n\n\n        tgt_mask = (~torch.tril(torch.ones((S, S), dtype=torch.bool))).to(DEVICE)\n\n        encoder_output = self.transformer.encoder(\n            src_embedded,\n            src_key_padding_mask=src_key_padding_mask\n        )\n\n        decoder_output = self.transformer.decoder(\n            tgt_embedded,\n            encoder_output,\n            tgt_mask=tgt_mask,\n            tgt_key_padding_mask=tgt_key_padding_mask,\n            memory_key_padding_mask=src_key_padding_mask\n        )\n\n        output = self.output_layer(decoder_output)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:25:40.479733Z","iopub.execute_input":"2025-04-02T13:25:40.480099Z","iopub.status.idle":"2025-04-02T13:25:40.582251Z","shell.execute_reply.started":"2025-04-02T13:25:40.480069Z","shell.execute_reply":"2025-04-02T13:25:40.581420Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"vocab_size_enc = tokenizer_en.get_vocab_size()\nvocab_size_dec = tokenizer_ru.get_vocab_size()\nembed_dim = 256\nnum_heads = 8\nff_dim = embed_dim*4\nnum_layers = 4\n\nbatch_size = 100","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:25:40.583220Z","iopub.execute_input":"2025-04-02T13:25:40.583574Z","iopub.status.idle":"2025-04-02T13:25:40.614401Z","shell.execute_reply.started":"2025-04-02T13:25:40.583540Z","shell.execute_reply":"2025-04-02T13:25:40.613817Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"model = TransformerEncoderDecoder(vocab_size_enc=tokenizer_ru.get_vocab_size(), vocab_size_dec=tokenizer_en.get_vocab_size(), embed_dim=256, num_heads=8, ff_dim=256*4, num_layers=4)\n\ntraining_set = Dataset(X_ru_train, X_en_train)\ntraining_generator = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True)\n\nvalid_set = Dataset(X_ru_valid, X_en_valid)\nvalid_generator = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:25:40.614996Z","iopub.execute_input":"2025-04-02T13:25:40.615236Z","iopub.status.idle":"2025-04-02T13:26:03.369979Z","shell.execute_reply.started":"2025-04-02T13:25:40.615216Z","shell.execute_reply":"2025-04-02T13:26:03.369293Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"from time import time\ndef train(model, iterator, optimizer, criterion, scheduler, print_every=100):\n\n    epoch_loss = []\n    ac = []\n\n    model.train()\n\n    for i, (texts_en, texts_ru) in enumerate(iterator):\n        texts_en = texts_en.to(DEVICE)\n        texts_ru = texts_ru.to(DEVICE)\n        texts_ru_input = texts_ru[:,:-1].to(DEVICE)\n        texts_ru_out = texts_ru[:, 1:].to(DEVICE)\n        src_padding_mask = (texts_en == PAD_IDX).to(DEVICE)\n        tgt_padding_mask = (texts_ru_input == PAD_IDX).to(DEVICE)\n\n\n        logits = model(texts_en, texts_ru_input, src_padding_mask, tgt_padding_mask)\n        optimizer.zero_grad()\n        B,S,C = logits.shape\n        loss = loss_fn(logits.reshape(B*S, C), texts_ru_out.reshape(B*S))\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        epoch_loss.append(loss.item())\n\n        if not (i+1) % print_every:\n            print(f'Loss: {np.mean(epoch_loss)};')\n\n    return np.mean(epoch_loss)\n\ndef evaluate(model, iterator, criterion):\n\n    epoch_loss = []\n    epoch_f1 = []\n\n    model.eval()\n    with torch.no_grad():\n        for i, (texts_en, texts_ru) in enumerate(iterator):\n            texts_en = texts_en.to(DEVICE)\n            texts_ru = texts_ru.to(DEVICE)\n            texts_ru_input = texts_ru[:,:-1].to(DEVICE)\n            texts_ru_out = texts_ru[:, 1:].to(DEVICE)\n            src_padding_mask = (texts_en == PAD_IDX).to(DEVICE)\n            tgt_padding_mask = (texts_ru_input == PAD_IDX).to(DEVICE)\n\n            logits = model(texts_en, texts_ru_input, src_padding_mask, tgt_padding_mask)\n\n            B,S,C = logits.shape\n            loss = loss_fn(logits.reshape(B*S, C), texts_ru_out.reshape(B*S))\n            epoch_loss.append(loss.item())\n\n    return np.mean(epoch_loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:26:03.370764Z","iopub.execute_input":"2025-04-02T13:26:03.370988Z","iopub.status.idle":"2025-04-02T13:26:03.380181Z","shell.execute_reply.started":"2025-04-02T13:26:03.370969Z","shell.execute_reply":"2025-04-02T13:26:03.379358Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"@torch.no_grad\ndef translate(text):\n\n    input_ids = tokenizer_ru.encode(text).ids[:max_len_en]\n    output_ids = [tokenizer_en.token_to_id('[BOS]')]\n\n    input_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(input_ids)], batch_first=True).to(DEVICE)\n    output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)], batch_first=True).to(DEVICE)\n\n    src_padding_mask = (input_ids_pad == PAD_IDX).to(DEVICE)\n    tgt_padding_mask = (output_ids_pad == PAD_IDX).to(DEVICE)\n\n    logits = model(input_ids_pad, output_ids_pad, src_padding_mask, tgt_padding_mask)\n\n    pred = logits.argmax(2).item()\n\n    while pred not in [tokenizer_en.token_to_id('[EOS]'), tokenizer_en.token_to_id('[PAD]')] and len(output_ids) < 100:\n        output_ids.append(pred)\n        output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)], batch_first=True).to(DEVICE)\n        tgt_padding_mask = (output_ids_pad == PAD_IDX).to(DEVICE)\n\n        logits = model(input_ids_pad, output_ids_pad, src_padding_mask, tgt_padding_mask)\n        pred = logits.argmax(2).view(-1)[-1].item()\n\n    return tokenizer_en.decoder.decode([tokenizer_en.id_to_token(i) for i in output_ids[1:]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:26:03.380920Z","iopub.execute_input":"2025-04-02T13:26:03.381140Z","iopub.status.idle":"2025-04-02T13:26:03.404398Z","shell.execute_reply.started":"2025-04-02T13:26:03.381092Z","shell.execute_reply":"2025-04-02T13:26:03.403756Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"model = model.to(DEVICE)\nloss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n\nNUM_EPOCHS = 20\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, pct_start=0.05,\n                                                steps_per_epoch=len(training_generator), epochs=NUM_EPOCHS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:26:03.405227Z","iopub.execute_input":"2025-04-02T13:26:03.405510Z","iopub.status.idle":"2025-04-02T13:26:03.475544Z","shell.execute_reply.started":"2025-04-02T13:26:03.405481Z","shell.execute_reply":"2025-04-02T13:26:03.474880Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"from timeit import default_timer as timer\n\nlosses = []\nfor epoch in range(1, NUM_EPOCHS+1):\n    start_time = timer()\n    train_loss = train(model, training_generator, optimizer, loss_fn, scheduler)\n    end_time = timer()\n    val_loss = evaluate(model, valid_generator, loss_fn)\n\n    if not losses:\n        print(f'First epoch - {val_loss}, saving model..')\n        torch.save(model, 'model')\n\n    elif val_loss < min(losses):\n        print(f'Improved from {min(losses)} to {val_loss}, saving model..')\n        torch.save(model, 'model')\n\n    losses.append(val_loss)\n\n    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \\\n           \"f\"Epoch time={(end_time-start_time):.3f}s\"))\n\n    print(translate(\"Солнце светит ярко сегодня.\"))\n    print(translate('Она читает книгу в саду.'))\n    print(translate('Вчера мы ходили в кино.'))\n    print(translate('Ты любишь путешествовать?'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:26:48.128349Z","iopub.execute_input":"2025-04-02T13:26:48.128711Z","iopub.status.idle":"2025-04-02T17:31:08.190613Z","shell.execute_reply.started":"2025-04-02T13:26:48.128682Z","shell.execute_reply":"2025-04-02T17:31:08.188712Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Loss: 9.223043956756591;\nLoss: 8.68271671295166;\nLoss: 8.243358964920043;\nLoss: 7.914159022569656;\nLoss: 7.664299178123474;\nLoss: 7.481049892902374;\nLoss: 7.327651720728193;\nLoss: 7.204285857677459;\nLoss: 7.098638204468621;\nLoss: 7.006090016841888;\nLoss: 6.92505360993472;\nLoss: 6.851592889229456;\nLoss: 6.783779351161076;\nLoss: 6.722773492676871;\nLoss: 6.665105518658956;\nLoss: 6.610862447321415;\nLoss: 6.559599141233107;\nLoss: 6.510222001340654;\nLoss: 6.46438165162739;\nLoss: 6.421387293338776;\nLoss: 6.380620034989857;\nLoss: 6.341961101835424;\nLoss: 6.30358838122824;\nLoss: 6.266447806755702;\nLoss: 6.2312002338409425;\nLoss: 6.196645984649658;\nLoss: 6.163078776995341;\nLoss: 6.129840738773346;\nLoss: 6.10002393393681;\nLoss: 6.070088535626729;\nLoss: 6.040658587332695;\nLoss: 6.01228588566184;\nLoss: 5.98508316487977;\nLoss: 5.95818591300179;\nLoss: 5.932806850024632;\nLoss: 5.907041537761688;\nLoss: 5.881873382491034;\nLoss: 5.8578438992249335;\nLoss: 5.833890034357707;\nLoss: 5.8103335326910015;\nLoss: 5.7875350027549555;\nLoss: 5.765656928561983;\nLoss: 5.743674122899078;\nLoss: 5.722456485249779;\nLoss: 5.701419896125794;\nLoss: 5.680549590691276;\nLoss: 5.660909626534645;\nLoss: 5.640894400974115;\nLoss: 5.621507928322773;\nLoss: 5.60232762928009;\nLoss: 5.584143666847079;\nLoss: 5.566526687787129;\nLoss: 5.548441549876951;\nLoss: 5.530954328996164;\nLoss: 5.514054028077559;\nLoss: 5.496998755335808;\nLoss: 5.480623915237293;\nLoss: 5.464146363159706;\nLoss: 5.448323540081412;\nLoss: 5.432651973128319;\nLoss: 5.417104548938939;\nLoss: 5.40172225848321;\nLoss: 5.386942939644769;\nLoss: 5.371952400617301;\nLoss: 5.3572829731060905;\nLoss: 5.342877682664177;\nLoss: 5.32885746966547;\nLoss: 5.315275251058972;\nLoss: 5.302070258078368;\nLoss: 5.288609140770776;\nLoss: 5.275065535793842;\nLoss: 5.261703621082836;\nLoss: 5.248820885044255;\nLoss: 5.236266908323443;\nLoss: 5.223789237944285;\nLoss: 5.211779363971008;\nLoss: 5.199537494678002;\nLoss: 5.187309720118841;\nLoss: 5.1755212268044675;\nLoss: 5.163744433999062;\nLoss: 5.152135059303707;\nLoss: 5.140842186445143;\nLoss: 5.129562593281987;\nLoss: 5.118536996245385;\nLoss: 5.107655091538149;\nLoss: 5.096748938948609;\nLoss: 5.085974165187485;\nLoss: 5.075622586607933;\nLoss: 5.065298177285141;\nLoss: 5.055065080112881;\nLoss: 5.045101028562902;\nLoss: 5.0351484923259076;\nLoss: 5.025625319865442;\nLoss: 5.015559986576121;\nLoss: 5.0054949927580985;\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:178.)\n  output = torch._nested_tensor_from_mask(\n","output_type":"stream"},{"name":"stdout","text":"First epoch - 3.9933562245368956, saving model..\nEpoch: 1, Train loss: 5.005, Val loss: 3.993,            Epoch time=1808.161s\nThe most important thing is the only one day .\nShe ' s in the house .\nWe ' ve been in the house .\nYou love a little ?\nLoss: 4.022424058914185;\nLoss: 4.006895552873612;\nLoss: 4.000445860226949;\nLoss: 3.9973084235191347;\nLoss: 3.997392204284668;\nLoss: 3.99490420182546;\nLoss: 3.993723681994847;\nLoss: 3.98845636844635;\nLoss: 3.986220279534658;\nLoss: 3.9846769127845763;\nLoss: 3.980241358930414;\nLoss: 3.9763664003213246;\nLoss: 3.9755400998775774;\nLoss: 3.9715638160705566;\nLoss: 3.9671533416112266;\nLoss: 3.9643355959653857;\nLoss: 3.9605833091455347;\nLoss: 3.956493777566486;\nLoss: 3.9520689365738315;\nLoss: 3.948674214839935;\nLoss: 3.9455024447895233;\nLoss: 3.942200259295377;\nLoss: 3.940243249976117;\nLoss: 3.937986590862274;\nLoss: 3.9332942165374756;\nLoss: 3.92935200544504;\nLoss: 3.927350755267673;\nLoss: 3.924936270884105;\nLoss: 3.922020130979604;\nLoss: 3.9191658143202464;\nLoss: 3.9162217910828128;\nLoss: 3.9137008208036423;\nLoss: 3.910742966045033;\nLoss: 3.907626829918693;\nLoss: 3.904950936113085;\nLoss: 3.901259317530526;\nLoss: 3.8970507400100294;\nLoss: 3.8934840224918568;\nLoss: 3.890245893612886;\nLoss: 3.8870132294893263;\nLoss: 3.8837162742963653;\nLoss: 3.881257385639917;\nLoss: 3.8784495342609495;\nLoss: 3.8757568156719207;\nLoss: 3.8727860158814322;\nLoss: 3.869482704193696;\nLoss: 3.8665039100038245;\nLoss: 3.8636789633333684;\nLoss: 3.8608654088876686;\nLoss: 3.858351738643646;\nLoss: 3.855849501104916;\nLoss: 3.8526046298558896;\nLoss: 3.8495266648958313;\nLoss: 3.8459150989850364;\nLoss: 3.8431515498594804;\nLoss: 3.8405727979540827;\nLoss: 3.837877395027562;\nLoss: 3.835297670035527;\nLoss: 3.8327243531356423;\nLoss: 3.829875742594401;\nLoss: 3.8273472807837314;\nLoss: 3.824766955144944;\nLoss: 3.8220631729988823;\nLoss: 3.819006116874516;\nLoss: 3.8162053114450893;\nLoss: 3.813391984087048;\nLoss: 3.810458526931592;\nLoss: 3.807882555337513;\nLoss: 3.8055109715116195;\nLoss: 3.8030353078842163;\nLoss: 3.7999871558538625;\nLoss: 3.7969300117426448;\nLoss: 3.7944628566258576;\nLoss: 3.7914002922418955;\nLoss: 3.788941790517171;\nLoss: 3.7866164899186083;\nLoss: 3.784111394541604;\nLoss: 3.781767769226661;\nLoss: 3.779217927123927;\nLoss: 3.7770150930583477;\nLoss: 3.7746661495279383;\nLoss: 3.7724533747172937;\nLoss: 3.7700871413299835;\nLoss: 3.7674196948891594;\nLoss: 3.7649660665568185;\nLoss: 3.762487021795539;\nLoss: 3.760131366554348;\nLoss: 3.7580710527300836;\nLoss: 3.755742793431443;\nLoss: 3.753123392290539;\nLoss: 3.750586696954874;\nLoss: 3.748052079833072;\nLoss: 3.745562812282193;\nLoss: 3.743218678540372;\nLoss: 3.740770734636407;\nImproved from 3.9933562245368956 to 3.4011765747070313, saving model..\nEpoch: 2, Train loss: 3.741, Val loss: 3.401,            Epoch time=1800.236s\nThe old lady is a good one .\nShe ' s in the bathroom .\nWe ' ve been in the movies .\nYou like a bus ?\nLoss: 3.399329402446747;\nLoss: 3.3933478784561157;\nLoss: 3.3967899314562477;\nLoss: 3.4048863464593886;\nLoss: 3.4112678661346436;\nLoss: 3.408535836935043;\nLoss: 3.4078607988357543;\nLoss: 3.4037493908405305;\nLoss: 3.4067312200864155;\nLoss: 3.408239150047302;\nLoss: 3.406277088468725;\nLoss: 3.4062519665559132;\nLoss: 3.406197289686937;\nLoss: 3.405936623811722;\nLoss: 3.4046685787836712;\nLoss: 3.4046729224920274;\nLoss: 3.40255239388522;\nLoss: 3.4006651193565793;\nLoss: 3.3998886340542844;\nLoss: 3.398651861310005;\nLoss: 3.3980585576239086;\nLoss: 3.397615316022526;\nLoss: 3.3978506406493807;\nLoss: 3.3971988544861476;\nLoss: 3.396869359970093;\nLoss: 3.39683226447839;\nLoss: 3.3952105573371605;\nLoss: 3.395591715829713;\nLoss: 3.394382067466604;\nLoss: 3.3932464129924775;\nLoss: 3.392813069435858;\nLoss: 3.391319701373577;\nLoss: 3.390825338941632;\nLoss: 3.3898926645166734;\nLoss: 3.3876340497561865;\nLoss: 3.3861271227730647;\nLoss: 3.385259132514129;\nLoss: 3.3844387260863655;\nLoss: 3.38346558595315;\nLoss: 3.382350079536438;\nLoss: 3.381180813021776;\nLoss: 3.3811577452932084;\nLoss: 3.380025232891704;\nLoss: 3.3789684665744955;\nLoss: 3.377914884355333;\nLoss: 3.37640876966974;\nLoss: 3.374803678938683;\nLoss: 3.3741663961609203;\nLoss: 3.373748048179004;\nLoss: 3.3721181846618653;\nLoss: 3.370656718787025;\nLoss: 3.369517478621923;\nLoss: 3.3682802757227197;\nLoss: 3.3672676992416384;\nLoss: 3.3660894432934847;\nLoss: 3.3644144185100284;\nLoss: 3.363499941993178;\nLoss: 3.362179429366671;\nLoss: 3.361015286243568;\nLoss: 3.3600672044754027;\nLoss: 3.351725467354504;\nLoss: 3.3505686403723325;\nLoss: 3.3497487587168595;\nLoss: 3.3482644735063825;\nLoss: 3.3474918122358726;\nLoss: 3.3463962590032152;\nLoss: 3.345338839962058;\nLoss: 3.3444456712619677;\nLoss: 3.3434062319755555;\nLoss: 3.342460395066362;\nLoss: 3.3415762615203857;\nLoss: 3.3405486470919388;\nLoss: 3.339440306621262;\nLoss: 3.3382740648388864;\nLoss: 3.3366290514263106;\nLoss: 3.335270667483167;\nLoss: 3.3340751371326216;\nLoss: 3.3328469570761636;\nLoss: 3.3316864999041838;\nLoss: 3.330751518288324;\nLoss: 3.329332205427104;\nLoss: 3.328221551532095;\nLoss: 3.3272446948490786;\nLoss: 3.325712145752377;\nLoss: 3.324609148659549;\nLoss: 3.32342943261499;\nLoss: 3.3222295881343142;\nLoss: 3.3212286758422853;\nLoss: 3.319885054236964;\nImproved from 3.4011765747070313 to 3.0896045351028443, saving model..\nEpoch: 3, Train loss: 3.320, Val loss: 3.090,            Epoch time=1800.791s\nThe Sun is being quiet today .\nShe ' s thinking about the book .\nYesterday we ' ve gone to the movies .\nYou love tickets ?\nLoss: 3.07556658744812;\nLoss: 3.08068546295166;\nLoss: 3.0873191809654235;\nLoss: 3.0812527990341185;\nLoss: 3.0775161442756653;\nLoss: 3.0797729345162708;\nLoss: 3.0817148460660664;\nLoss: 3.083123896420002;\nLoss: 3.083166101243761;\nLoss: 3.0845697796344758;\nLoss: 3.0871214389801027;\nLoss: 3.0881715977191924;\nLoss: 3.089062338058765;\nLoss: 3.0890072458130975;\nLoss: 3.0904714697202045;\nLoss: 3.0931032423675062;\nLoss: 3.094561787913827;\nLoss: 3.0934980475902556;\nLoss: 3.0928274221169323;\nLoss: 3.092806562662125;\nLoss: 3.0915784569013685;\nLoss: 3.089452672329816;\nLoss: 3.0886853784063586;\nLoss: 3.08848402351141;\nLoss: 3.0875904898643496;\nLoss: 3.0882956348015713;\nLoss: 3.088207089724364;\nLoss: 3.0888817668812614;\nLoss: 3.0884302764925464;\nLoss: 3.0887009651660917;\nLoss: 3.087726510493986;\nLoss: 3.087541550844908;\nLoss: 3.086228603377487;\nLoss: 3.085696969663396;\nLoss: 3.0851425809179034;\nLoss: 3.085122030377388;\nLoss: 3.0851118776604936;\nLoss: 3.084662840554589;\nLoss: 3.0844012579551108;\nLoss: 3.0843724681138993;\nLoss: 3.083987877950436;\nLoss: 3.083855751525788;\nLoss: 3.0835159655504447;\nLoss: 3.0832806885784323;\nLoss: 3.0821050576104057;\nLoss: 3.0814044373968374;\nLoss: 3.080562530476996;\nLoss: 3.0800319012006123;\nLoss: 3.07944179768465;\nLoss: 3.078715119552612;\nLoss: 3.077880501232895;\nLoss: 3.077629785033373;\nLoss: 3.076544829629502;\nLoss: 3.076234241944772;\nLoss: 3.076025299982591;\nLoss: 3.0757949594088965;\nLoss: 3.074885326310208;\nLoss: 3.0742679024975876;\nLoss: 3.0731968275571275;\nLoss: 3.0727669191360474;\nLoss: 3.0725070663358345;\nLoss: 3.0717543143226256;\nLoss: 3.071425723688943;\nLoss: 3.070950446873903;\nLoss: 3.070052322974572;\nLoss: 3.0693942148396465;\nLoss: 3.069136801335349;\nLoss: 3.068460503220558;\nLoss: 3.064228162066142;\nLoss: 3.0637985286273453;\nLoss: 3.0629539396855736;\nLoss: 3.0621044678871447;\nLoss: 3.061220710639712;\nLoss: 3.0606423565745353;\nLoss: 3.060047121901571;\nLoss: 3.0595412454663253;\nLoss: 3.0588565830437533;\nLoss: 3.0582535586470647;\nLoss: 3.0575147762859567;\nLoss: 3.057001045748245;\nLoss: 3.0562538878122965;\nLoss: 3.055930805450136;\nLoss: 3.0550688899500984;\nLoss: 3.054469053665797;\nLoss: 3.0540192899075183;\nLoss: 3.053407304390617;\nLoss: 3.0524406915326274;\nLoss: 3.0516812332386665;\nLoss: 3.051132727497502;\nImproved from 3.0896045351028443 to 2.87855757188797, saving model..\nEpoch: 4, Train loss: 3.051, Val loss: 2.879,            Epoch time=1800.488s\nThe sun ' s warm today .\nShe reads the book .\nYesterday we went to the movies .\nYou like the trip ?\nLoss: 2.843965001106262;\nLoss: 2.834632222652435;\nLoss: 2.853091481526693;\nLoss: 2.8548618501424787;\nLoss: 2.858544933319092;\nLoss: 2.8553954231739045;\nLoss: 2.8582180970055715;\nLoss: 2.859813284277916;\nLoss: 2.8586247165997825;\nLoss: 2.8592889239788057;\nLoss: 2.8598151894049213;\nLoss: 2.8622322044769923;\nLoss: 2.864334551004263;\nLoss: 2.8645213297435217;\nLoss: 2.8659752046267193;\nLoss: 2.866726565659046;\nLoss: 2.8677651870951935;\nLoss: 2.866885522339079;\nLoss: 2.8667463753097935;\nLoss: 2.868085352540016;\nLoss: 2.8691752172651745;\nLoss: 2.8705746522816744;\nLoss: 2.870127086639404;\nLoss: 2.8702957813938457;\nLoss: 2.870671908760071;\nLoss: 2.871259389932339;\nLoss: 2.8695993385170446;\nLoss: 2.868962494906257;\nLoss: 2.8689670036179677;\nLoss: 2.8680031660530303;\nLoss: 2.8682013779073148;\nLoss: 2.8687326259989487;\nLoss: 2.8684085663771017;\nLoss: 2.8687381219267847;\nLoss: 2.868339197403047;\nLoss: 2.868474069549924;\nLoss: 2.8680125010845274;\nLoss: 2.868023106184873;\nLoss: 2.8679400137265523;\nLoss: 2.8676582197521046;\nLoss: 2.867401273960763;\nLoss: 2.867920094629129;\nLoss: 2.867430339832695;\nLoss: 2.8671253188610075;\nLoss: 2.8671835569774404;\nLoss: 2.8668608414668304;\nLoss: 2.866565411360759;\nLoss: 2.8664522670374977;\nLoss: 2.866002769123424;\nLoss: 2.8657338084919113;\nLoss: 2.865346951986614;\nLoss: 2.8651653744845555;\nLoss: 2.865063151424214;\nLoss: 2.8650817322333655;\nLoss: 2.864860146827385;\nLoss: 2.8646358872613598;\nLoss: 2.8645810847055344;\nLoss: 2.8642002909630535;\nLoss: 2.8639206789090084;\nLoss: 2.8632789906949707;\nLoss: 2.8633480672338116;\nLoss: 2.862560393179164;\nLoss: 2.8626490840013474;\nLoss: 2.862377408674785;\nLoss: 2.862133408230795;\nLoss: 2.8619283002614977;\nLoss: 2.861698194725873;\nLoss: 2.861295340641125;\nLoss: 2.860886210950216;\nLoss: 2.860316405233584;\nLoss: 2.859759437883055;\nLoss: 2.8580396060484;\nLoss: 2.8576055358421235;\nLoss: 2.85715225463755;\nLoss: 2.856732339942178;\nLoss: 2.8560328979601803;\nLoss: 2.8553979766639794;\nLoss: 2.855299399515216;\nLoss: 2.855051325506634;\nLoss: 2.85478453036193;\nLoss: 2.8545123418776885;\nLoss: 2.854301057784788;\nLoss: 2.853897683138543;\nLoss: 2.853399568507546;\nImproved from 2.87855757188797 to 2.7375536975860597, saving model..\nEpoch: 5, Train loss: 2.853, Val loss: 2.738,            Epoch time=1799.949s\nThe sun ' s coming today .\nShe reads a book in the garden .\nYesterday we went to the movies .\nYou like traveling ?\nLoss: 2.682696599960327;\nLoss: 2.681828541755676;\nLoss: 2.6818308917681377;\nLoss: 2.6818558371067045;\nLoss: 2.6810197014808654;\nLoss: 2.6884464812278748;\nLoss: 2.686601423536028;\nLoss: 2.6893901959061624;\nLoss: 2.690804268254174;\nLoss: 2.6917216091156004;\nLoss: 2.6948111924258145;\nLoss: 2.6962840288877485;\nLoss: 2.697582980119265;\nLoss: 2.698285972901753;\nLoss: 2.6976521382331846;\nLoss: 2.697908116132021;\nLoss: 2.6988580389583814;\nLoss: 2.6998035803106095;\nLoss: 2.6993577556861075;\nLoss: 2.699367443919182;\nLoss: 2.7007467026937575;\nLoss: 2.7008203224702316;\nLoss: 2.7002654633314713;\nLoss: 2.7005576878786086;\nLoss: 2.7015907390594482;\nLoss: 2.70225753564101;\nLoss: 2.7020130026782;\nLoss: 2.702000721352441;\nLoss: 2.7016932935550293;\nLoss: 2.7016515163580577;\nLoss: 2.702635460361358;\nLoss: 2.7029208190739156;\nLoss: 2.702993998672023;\nLoss: 2.703309516135384;\nLoss: 2.7028198103223526;\nLoss: 2.7031114937199487;\nLoss: 2.7036918154922693;\nLoss: 2.7034925562457035;\nLoss: 2.704164771238963;\nLoss: 2.7051866473555566;\nLoss: 2.7053342705819663;\nLoss: 2.705608508984248;\nLoss: 2.7058272295219954;\nLoss: 2.7057853816856037;\nLoss: 2.705464885075887;\nLoss: 2.706112821413123;\nLoss: 2.705864317975146;\nLoss: 2.7062496388951938;\nLoss: 2.706067889661205;\nLoss: 2.7064943307876588;\nLoss: 2.7067912228902182;\nLoss: 2.7069627650425985;\nLoss: 2.7072348040004948;\nLoss: 2.7074106897689676;\nLoss: 2.708038797075098;\nLoss: 2.708154999486038;\nLoss: 2.707825113848636;\nLoss: 2.707330868367491;\nLoss: 2.7068717878955906;\nLoss: 2.7054906447367233;\nLoss: 2.7056682183849277;\nLoss: 2.7054125252190757;\nLoss: 2.7054049129071442;\nLoss: 2.705234061104911;\nLoss: 2.7052433734208767;\nLoss: 2.704783251583576;\nLoss: 2.7048178391587245;\nLoss: 2.7045612636127987;\nLoss: 2.7043395652453106;\nLoss: 2.7043280996774373;\nLoss: 2.70439235643907;\nLoss: 2.704380634258955;\nLoss: 2.704244398768944;\nLoss: 2.7039331344366073;\nLoss: 2.7035753234816187;\nLoss: 2.703392886563045;\nLoss: 2.703221471367112;\nLoss: 2.7034232521908623;\nLoss: 2.7034377178304334;\nLoss: 2.703157594231672;\nLoss: 2.7029803702475013;\nLoss: 2.702573754733259;\nLoss: 2.7023592150613163;\nLoss: 2.7020229788886176;\nLoss: 2.702058227245624;\nLoss: 2.701781112422114;\nLoss: 2.7015331042453807;\nLoss: 2.701334643161043;\nLoss: 2.7011699793966195;\nImproved from 2.7375536975860597 to 2.6181832780838015, saving model..\nEpoch: 6, Train loss: 2.701, Val loss: 2.618,            Epoch time=1798.765s\nThe sun ' s bright today .\nShe ' s reading the book in the garden .\nWe went to the movies yesterday .\nYou love travelling ?\nLoss: 2.548006100654602;\nLoss: 2.548493320941925;\nLoss: 2.5470156860351563;\nLoss: 2.543415414094925;\nLoss: 2.5432030959129333;\nLoss: 2.5468249587217966;\nLoss: 2.5496196229117256;\nLoss: 2.552670416533947;\nLoss: 2.5532398547066584;\nLoss: 2.5547897362709047;\nLoss: 2.5558321048996664;\nLoss: 2.5567544958988826;\nLoss: 2.5569031873116126;\nLoss: 2.5575914531094686;\nLoss: 2.558702423254649;\nLoss: 2.559309663027525;\nLoss: 2.5594489184547875;\nLoss: 2.5612525657812752;\nLoss: 2.5619450667029935;\nLoss: 2.561528832435608;\nLoss: 2.5627726118905203;\nLoss: 2.5629855847358702;\nLoss: 2.5640275404764257;\nLoss: 2.5648169897993407;\nLoss: 2.566046142578125;\nLoss: 2.5668059758956616;\nLoss: 2.5679267095636438;\nLoss: 2.568036210707256;\nLoss: 2.5679811760474895;\nLoss: 2.5682391471862793;\nLoss: 2.568276304429577;\nLoss: 2.568715527430177;\nLoss: 2.5692738048958055;\nLoss: 2.569514377748265;\nLoss: 2.569292102609362;\nLoss: 2.5708030404647193;\nLoss: 2.5709300472285297;\nLoss: 2.5714920417258615;\nLoss: 2.5724777637383878;\nLoss: 2.5737467465996744;\nLoss: 2.573407323883801;\nLoss: 2.572986647344771;\nLoss: 2.572969765663147;\nLoss: 2.5733148259466345;\nLoss: 2.5733343323601616;\nLoss: 2.5738768701967984;\nLoss: 2.574684974284882;\nLoss: 2.574835455020269;\nLoss: 2.5753105065287376;\nLoss: 2.5758229124546053;\nLoss: 2.5760573235212587;\nLoss: 2.575838935833711;\nLoss: 2.5753502107116413;\nLoss: 2.575169536670049;\nLoss: 2.5752381327802483;\nLoss: 2.575150500025068;\nLoss: 2.574972463281531;\nLoss: 2.5751512183814214;\nLoss: 2.5749459840079485;\nLoss: 2.575017445087433;\nLoss: 2.574963987733497;\nLoss: 2.574870893493775;\nLoss: 2.57517051325904;\nLoss: 2.575188526287675;\nLoss: 2.575154319543105;\nLoss: 2.575383548772696;\nLoss: 2.5750891551579516;\nLoss: 2.5754489699247722;\nLoss: 2.5756545790672303;\nLoss: 2.5761266822563975;\nLoss: 2.5759585446506352;\nLoss: 2.5762034748150753;\nLoss: 2.5760606577728367;\nLoss: 2.5763592126965524;\nLoss: 2.576440377500322;\nLoss: 2.5760753327753485;\nLoss: 2.5758064451562355;\nLoss: 2.575805704224677;\nLoss: 2.575912195738624;\nLoss: 2.575813539333122;\nLoss: 2.57592634579231;\nLoss: 2.5760399970954113;\nLoss: 2.5755655427193376;\nLoss: 2.5755785239802464;\nLoss: 2.5754458668205764;\nLoss: 2.5754031734362894;\nLoss: 2.419094913005829;\nLoss: 2.425404878258705;\nLoss: 2.4256213002204894;\nLoss: 2.426047234336535;\nLoss: 2.4270179416452136;\nLoss: 2.4261733420193194;\nLoss: 2.4294252672460344;\nLoss: 2.431234217762947;\nLoss: 2.433040747534145;\nLoss: 2.4352929120262465;\nLoss: 2.43770910180532;\nLoss: 2.4398471705402645;\nLoss: 2.4404635236263275;\nLoss: 2.4413086763769387;\nLoss: 2.4415047644867616;\nLoss: 2.444447420901722;\nLoss: 2.445752078796688;\nLoss: 2.4468540256619455;\nLoss: 2.4493284740334467;\nLoss: 2.44969850090417;\nLoss: 2.450483156753623;\nLoss: 2.449840328345696;\nLoss: 2.450907897424698;\nLoss: 2.4514949957682535;\nLoss: 2.4523677657710183;\nLoss: 2.453981112795217;\nLoss: 2.453989958228736;\nLoss: 2.454880877614021;\nLoss: 2.454989127228337;\nLoss: 2.4555028899386526;\nLoss: 2.455896190549388;\nLoss: 2.456416473143241;\nLoss: 2.4565112458297182;\nLoss: 2.4567897461189165;\nLoss: 2.4563436316477287;\nLoss: 2.456574748597647;\nLoss: 2.4571039104156007;\nLoss: 2.4582769632041455;\nLoss: 2.458840527621711;\nLoss: 2.459778046011925;\nLoss: 2.4597082879654195;\nLoss: 2.460361591604623;\nLoss: 2.461216787841585;\nLoss: 2.463025648383533;\nLoss: 2.4630326165832006;\nLoss: 2.4633097254780103;\nLoss: 2.4635270985629822;\nLoss: 2.4639118582768873;\nLoss: 2.4641148921421596;\nLoss: 2.4640644281788875;\nLoss: 2.4642444342580334;\nLoss: 2.4644173013153723;\nLoss: 2.464887884259224;\nLoss: 2.465466652776374;\nLoss: 2.4657532905378647;\nLoss: 2.4655262822196597;\nLoss: 2.465629176646471;\nLoss: 2.465668014746446;\nLoss: 2.465656393730279;\nLoss: 2.465344817211379;\nLoss: 2.465566383021719;\nLoss: 2.4657217719243922;\nLoss: 2.4658217603138515;\nLoss: 2.4657416491777124;\nLoss: 2.4658881396055223;\nLoss: 2.466090767089635;\nLoss: 2.466415057182312;\nLoss: 2.466663672765096;\nLoss: 2.4666532977944926;\nLoss: 2.466660500130096;\nLoss: 2.4664109041293463;\nLoss: 2.4664796832694282;\nLoss: 2.466577398404479;\nLoss: 2.466391038291248;\nLoss: 2.4662958173490153;\nLoss: 2.4665190213559622;\nLoss: 2.466754888608342;\nLoss: 2.466802259361043;\nLoss: 2.4667806314590366;\nLoss: 2.467075579961141;\nLoss: 2.4673430575024;\nLoss: 2.46723826009236;\nLoss: 2.467042944431305;\nLoss: 2.467099237311017;\nLoss: 2.467009613565777;\nLoss: 2.467486827142777;\nLoss: 2.46705175280571;\nLoss: 2.4670222322940827;\nImproved from 2.5324736127853393 to 2.46022581243515, saving model..\nEpoch: 8, Train loss: 2.467, Val loss: 2.460,            Epoch time=1801.155s\nThe sun ' s bright today .\nShe reads a book in the garden .\nYesterday we went to the movies .\nYou like travelling ?\nLoss: 2.3377008700370787;\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-46-00720905f1f5>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-43-c74ebd081183>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, scheduler, print_every)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts_ru_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-40-d1fc82ed17b9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_key_padding_mask, tgt_key_padding_mask)\u001b[0m\n\u001b[1;32m     38\u001b[0m         )\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         decoder_output = self.transformer.decoder(\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mtgt_embedded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_seq_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mtgt_is_causal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_detect_is_causal_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_is_causal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_detect_is_causal_mask\u001b[0;34m(mask, is_causal, size)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0;31m# broadcasting the comparison.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcausal_comparison\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0mmake_causal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcausal_comparison\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m             \u001b[0mmake_causal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":46},{"cell_type":"markdown","source":"### BLEU","metadata":{}},{"cell_type":"code","source":"text = open('opus.en-ru-test.ru').read().replace('\\xa0', ' ')\nf = open('opus.en-ru-test.ru', 'w')\nf.write(text)\nf.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:17:55.631408Z","iopub.execute_input":"2025-04-02T13:17:55.631759Z","iopub.status.idle":"2025-04-02T13:17:55.662779Z","shell.execute_reply.started":"2025-04-02T13:17:55.631734Z","shell.execute_reply":"2025-04-02T13:17:55.661854Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"en_sents_test = open('opus.en-ru-test.en').read().splitlines()[:100]\nru_sents_test = open('opus.en-ru-test.ru').read().splitlines()[:100]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:17:57.369706Z","iopub.execute_input":"2025-04-02T13:17:57.369985Z","iopub.status.idle":"2025-04-02T13:17:57.377776Z","shell.execute_reply.started":"2025-04-02T13:17:57.369964Z","shell.execute_reply":"2025-04-02T13:17:57.376928Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"bleu_scores = []\ntotal_bleu = 0\n\nfor i in tqdm(range(len(ru_sents_test))):\n    gold = word_tokenize(en_sents_test[i])\n    pred = word_tokenize(translate(ru_sents_test[i]))\n    bleu_score = nltk.translate.bleu_score.sentence_bleu([gold], pred, auto_reweigh=True)\n    bleu_scores.append((bleu_score, ru_sents_test[i], en_sents_test[i], pred))\n    total_bleu += bleu_score\n\naverage_bleu = total_bleu / len(ru_sents_test)\naverage_bleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T18:12:04.773383Z","iopub.execute_input":"2025-04-02T18:12:04.773719Z","iopub.status.idle":"2025-04-02T18:12:21.288951Z","shell.execute_reply.started":"2025-04-02T18:12:04.773691Z","shell.execute_reply":"2025-04-02T18:12:21.288189Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 100/100 [00:16<00:00,  6.06it/s]\n","output_type":"stream"},{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"0.3786726138662766"},"metadata":{}}],"execution_count":58},{"cell_type":"code","source":"bleu_scores.sort(reverse=True, key=lambda x: x[0])\nfor (score, ru_sent, en_sent, pred) in bleu_scores[:5]:\n    print(score)\n    print(ru_sent)\n    print(en_sent)\n    print(pred)\n    print('--------------------')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T18:17:41.320200Z","iopub.execute_input":"2025-04-02T18:17:41.320517Z","iopub.status.idle":"2025-04-02T18:17:41.329862Z","shell.execute_reply.started":"2025-04-02T18:17:41.320491Z","shell.execute_reply":"2025-04-02T18:17:41.329067Z"}},"outputs":[{"name":"stdout","text":"1.0\n12844\n12844\n['12844']\n--------------------\n0.7598356856515925\nОни удручают ещё больше.\nThey're more depressing.\n['They', \"'\", 're', 'still', 'breathing', '.']\n--------------------\n0.7598356856515925\nНо разочарование «Хезболлы» превратилось в интенсивноебеспокойство, когда сирийцы восстали против Асада.\nBut Hezbollah’s disappointment turned to intense concernwhen Syrians rebelled against Assad.\n['But', 'the', '“', 'Hezbollah', '”', 'turned', 'into', 'intense', 'concern', 'when', 'Syrian', 'settlers', 'became', 'a', 'fight', 'against', 'Aubarak', '.']\n--------------------\n0.7311104457090247\nИ как ты только справляешься, папа, таская эти коробки взад-вперед целый день.\nI don't know how you do it, Pop, carrying these boxes around every day.\n['And', 'as', 'soon', 'as', 'you', \"'\", 're', 'only', 'doing', ',', 'Dad', ',', 'these', 'are', 'the', 'boxes', 'of', 'the', 'box', 'forward', '.']\n--------------------\n0.7311104457090247\nКоллекция администрации Адамса.\nA collection from the Adams administration.\n['The', 'Collection', 'of', 'the', 'Addison', 'Administration', '.']\n--------------------\n","output_type":"stream"}],"execution_count":60},{"cell_type":"markdown","source":"Некоторые предложения все-таки совсем неправильно перевелись (например, второе). Но, например, более длинные предложения как будто бы правда более менее переводятся нормально, и видно, что модель заучила какие-то конструкции, но возможно не всегда их правильно пока применяет.","metadata":{}},{"cell_type":"markdown","source":"### Translate","metadata":{}},{"cell_type":"code","source":"@torch.no_grad\ndef translate(texts):\n    input_ids = [tokenizer_ru.encode(text).ids[:max_len_en] for text in texts]\n    output_ids = [[tokenizer_en.token_to_id('[BOS]')] for _ in texts]\n\n    input_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(ids) for ids in input_ids], batch_first=True).to(DEVICE)\n    output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(ids) for ids in output_ids], batch_first=True).to(DEVICE)\n\n    src_padding_mask = (input_ids_pad == PAD_IDX).to(DEVICE)\n    tgt_padding_mask = (output_ids_pad == PAD_IDX).to(DEVICE)\n\n    logits = model(input_ids_pad, output_ids_pad, src_padding_mask, tgt_padding_mask)\n\n    preds = logits.argmax(2)[:, 0].unsqueeze(1)\n\n    done_flags = [False] * len(texts)\n    max_len = 100\n\n    while not all(done_flags):\n        for i in range(len(texts)):\n            if not done_flags[i]:\n                output_ids[i].append(preds[i].item())\n\n        output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(ids) for ids in output_ids], batch_first=True).to(DEVICE)\n        tgt_padding_mask = (output_ids_pad == PAD_IDX).to(DEVICE)\n\n        logits = model(input_ids_pad, output_ids_pad, src_padding_mask, tgt_padding_mask)\n\n        preds = logits.argmax(2)[:, -1]\n\n        for i in range(len(texts)):\n            if not done_flags[i]:\n                if preds[i].item() == tokenizer_en.token_to_id('[EOS]') or len(output_ids[i]) >= max_len:\n                    done_flags[i] = True\n\n    translated_texts = [tokenizer_en.decoder.decode([tokenizer_en.id_to_token(i) for i in output_ids[i][1:]]) for i in range(len(texts))]\n\n    return translated_texts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T18:39:25.658955Z","iopub.execute_input":"2025-04-02T18:39:25.659354Z","iopub.status.idle":"2025-04-02T18:39:25.668253Z","shell.execute_reply.started":"2025-04-02T18:39:25.659323Z","shell.execute_reply":"2025-04-02T18:39:25.667431Z"}},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":"Проверим, что на 100 такие же результаты:","metadata":{}},{"cell_type":"code","source":"en_sents_test = open('opus.en-ru-test.en').read().splitlines()[:100]\nru_sents_test = open('opus.en-ru-test.ru').read().splitlines()[:100]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T18:51:51.858295Z","iopub.execute_input":"2025-04-02T18:51:51.858708Z","iopub.status.idle":"2025-04-02T18:51:51.866619Z","shell.execute_reply.started":"2025-04-02T18:51:51.858678Z","shell.execute_reply":"2025-04-02T18:51:51.865877Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"batch_size = 32\nbleu_scores = []\ntotal_bleu = 0\n\ndef batchify(texts, batch_size):\n    for i in range(0, len(texts), batch_size):\n        yield texts[i:i + batch_size]\n\nfor batch_start in tqdm(range(0, len(ru_sents_test), batch_size)):\n    batch_ru = ru_sents_test[batch_start:batch_start + batch_size]\n    batch_en = en_sents_test[batch_start:batch_start + batch_size]\n\n    batch_pred = translate(batch_ru)\n\n    for (ru_sent, en_sent, pred) in zip(batch_ru, batch_en, batch_pred):\n        gold = word_tokenize(en_sent)\n        pred = word_tokenize(pred)\n        bleu_score = nltk.translate.bleu_score.sentence_bleu([gold], pred, auto_reweigh=True)\n        \n        bleu_scores.append((bleu_score, ru_sent, en_sent, pred))\n        total_bleu += bleu_score\n\naverage_bleu = total_bleu / len(ru_sents_test)\naverage_bleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T18:51:53.260540Z","iopub.execute_input":"2025-04-02T18:51:53.260847Z","iopub.status.idle":"2025-04-02T18:51:55.831014Z","shell.execute_reply.started":"2025-04-02T18:51:53.260821Z","shell.execute_reply":"2025-04-02T18:51:55.830084Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 4/4 [00:02<00:00,  1.56it/s]\n","output_type":"stream"},{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"0.37835365738321547"},"metadata":{}}],"execution_count":70},{"cell_type":"markdown","source":"Теперь на полном:","metadata":{}},{"cell_type":"code","source":"en_sents_test = open('opus.en-ru-test.en').read().splitlines()\nru_sents_test = open('opus.en-ru-test.ru').read().splitlines()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T18:40:42.830541Z","iopub.execute_input":"2025-04-02T18:40:42.830848Z","iopub.status.idle":"2025-04-02T18:40:42.838188Z","shell.execute_reply.started":"2025-04-02T18:40:42.830824Z","shell.execute_reply":"2025-04-02T18:40:42.837383Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"batch_size = 32\nbleu_scores = []\ntotal_bleu = 0\n\ndef batchify(texts, batch_size):\n    for i in range(0, len(texts), batch_size):\n        yield texts[i:i + batch_size]\n\nfor batch_start in tqdm(range(0, len(ru_sents_test), batch_size)):\n    batch_ru = ru_sents_test[batch_start:batch_start + batch_size]\n    batch_en = en_sents_test[batch_start:batch_start + batch_size]\n\n    batch_pred = translate(batch_ru)\n\n    for (ru_sent, en_sent, pred) in zip(batch_ru, batch_en, batch_pred):\n        gold = word_tokenize(en_sent)\n        pred = word_tokenize(pred)\n        bleu_score = nltk.translate.bleu_score.sentence_bleu([gold], pred, auto_reweigh=True)\n        \n        bleu_scores.append((bleu_score, ru_sent, en_sent, pred))\n        total_bleu += bleu_score\n\naverage_bleu = total_bleu / len(ru_sents_test)\naverage_bleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T18:50:20.485926Z","iopub.execute_input":"2025-04-02T18:50:20.486285Z","iopub.status.idle":"2025-04-02T18:51:08.066752Z","shell.execute_reply.started":"2025-04-02T18:50:20.486261Z","shell.execute_reply":"2025-04-02T18:51:08.065779Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 63/63 [00:47<00:00,  1.32it/s]\n","output_type":"stream"},{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"0.40471688852092874"},"metadata":{}}],"execution_count":68},{"cell_type":"markdown","source":"### Back Translation","metadata":{}},{"cell_type":"markdown","source":"Back translation — это техника, которая помогает улучшить модель машинного перевода, когда параллельных данных недостаточно. Суть в том, чтобы взять уже существующие переводы с одного языка на другой и перевести их обратно, создавая дополнительные примеры для обучения модели. Этот метод позволяет использовать одноязычные тексты для создания синтетических пар данных, что особенно полезно, когда реальных параллельных данных не хватает. В отличие от традиционного подхода, при котором используется только существующий параллельный корпус, back translation расширяет обучающий набор, генерируя дополнительные переводы, которые могут быть использованы для обучения модели.\n\nДля применения этой техники нужно сначала обучить модель перевода в одном направлении, например, как в семинаре, с английского на русский. После этого тренируется модель для обратного перевода с русского на английский. Затем, используя модель ru -> en, переводим одноязычные русские тексты на английский. Это даёт нам новые синтетические пары данных: оригинальные русские предложения и переведённые обратно на английский. Эти новые пары можно добавить в параллельный корпус.\n\nЧтобы применить back translation, нужно дважды обучить модели: одну для перевода с английского на русский и другую для обратного перевода. Всего потребуется минимум два запуска обучения — первый для начальной модели и второй с новыми данными, полученными через back translation.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
